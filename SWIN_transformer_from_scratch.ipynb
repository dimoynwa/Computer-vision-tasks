{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNniWeAOEaPev64M+GiOnQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimoynwa/Computer-vision-tasks/blob/main/SWIN_transformer_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SWIN transformers from scracth\n",
        "\n",
        "Implement SWIN transformers from scratch using Pytorch.\n",
        "\n",
        "Following: https://github.com/berniwal/swin-transformer-pytorch\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=16xWyp2Q5oio-m8GtuaAdCZBRT4wTwJeF)\n"
      ],
      "metadata": {
        "id": "BadWwZCL0-L-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvCkmr_G06Mz",
        "outputId": "929d86a4-2a7b-43ac-b874-d91a0863c3a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m702.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, einsum\n",
        "import numpy as np\n",
        "\n",
        "!pip install einops # Clear and reliable tensor transformations\n",
        "from einops import rearrange\n",
        "from einops import einsum\n",
        "\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create classes\n",
        "\n",
        "Every **Stage** at the image is represented in the `StageModule` class.\n",
        "What it does? First is going to do **Patch Merging**. The idea of Patch Merging is just to change the size and create a hierarchy. The output of PatchMerging at Stage 2 will be [b_s, 28, 28, 192]. The Patch Merging is implemented in `PatchMerging_Conv` class. And also the **output** of the SwinTransformerBlock at Stage 2 will be the same [b_s, 28, 28, 192]. **Input** and **Output** of **SwinTransformerBlock** are the same shapes.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1UkEOxgGyd8MK9u1fyQ9oRwluykv-1aea)\n"
      ],
      "metadata": {
        "id": "qepIH2FT3Oij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging_Conv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, downscalling_factor):\n",
        "    super().__init__()\n",
        "    self.patch_merging = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                   kernel_size=downscalling_factor,\n",
        "                                   stride=downscalling_factor,\n",
        "                                   padding=0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print(f'x.size = {x.size()}').  # (1, (3, 96, 192, 384), (224, 56, 28, 14), (224, 56, 28, 14))\n",
        "    # self.patch_merging(x).   # (1, (96, 192, 384, 768), (56, 28, 14, 7), (56, 28, 14, 7))\n",
        "    x = self.patch_merging(x).permute(0, 2, 3, 1) # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "    return x"
      ],
      "metadata": {
        "id": "jAUXv2P_02W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PatchMerging different approach\n",
        "\n",
        "To remove **Conv2d** layer we can use different approach for downscalling the image and increasing the number of channels using `torch.nn.Unfold` and `torch.nn.Linear`.  "
      ],
      "metadata": {
        "id": "HkCLfdSVx8sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, downscalling_factor):\n",
        "    super().__init__()\n",
        "    self.downscalling_factor = downscalling_factor\n",
        "\n",
        "    self.patch_merge = nn.Unfold(kernel_size=downscalling_factor,\n",
        "                                 stride=downscalling_factor)\n",
        "    self.linear = nn.Linear(in_channels * downscalling_factor ** 2, out_channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, c, h, w = x.shape\n",
        "    # print(f'x.size(): {x.size()}')      # (1, (3, 96, 192, 384), (224, 56, 28, 14), (224, 56, 28, 14))\n",
        "\n",
        "    new_h, new_w = h // self.downscalling_factor, w // self.downscalling_factor\n",
        "    # print(f'new_h: {new_h}, new_w: {new_w}')    # (56, 28, 14, 7)\n",
        "\n",
        "    tmp_patch_merge = self.patch_merge(x) # (1, (48, 384, 768, 1536), (3136, 768, 196, 49))\n",
        "    # print(f'tmp_patch_merge.size(): {tmp_patch_merge.size()}')\n",
        "\n",
        "    tmp_view = self.patch_merge(x).view(b, -1, new_h, new_w) # (1, (48, 384, 768, 1536), (56, 28, 14, 7), (56, 28, 14, 7))\n",
        "    # print(f'tmp_view.size(): {tmp_view.size()}')\n",
        "\n",
        "    x = self.patch_merge(x).view(b, -1, new_h, new_w) # (1, (48, 384, 768, 1536), (56, 28, 14, 7), (56, 28, 14, 7))\n",
        "    # print(f'x.size(): {x.size()}')\n",
        "    x = x.permute(0, 2, 3, 1) # (1, (56, 28, 14, 7), (56, 28, 14, 7), (48, 384, 768, 1536))\n",
        "    # print(f'x.size(): {x.size()}')\n",
        "\n",
        "    x = self.linear(x) # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "    # print(f'x.size(): {x.size()}')\n",
        "\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OniW4lDtx78v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test PatchMerging_Conv"
      ],
      "metadata": {
        "id": "U77bb1DAn4kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_in = torch.randn(2, 3, 224, 224)\n",
        "pm1 = PatchMerging_Conv(3, 96, 4)\n",
        "pm2 = PatchMerging_Conv(96, 192, 2)\n",
        "pm3 = PatchMerging_Conv(192, 384, 2)\n",
        "\n",
        "out = pm1(dummy_in)\n",
        "print(f'Out shape after PatchMerging_1: {out.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hjyrk_Fn37m",
        "outputId": "9e957f6e-dc17-4d78-8ba6-5a46cf1bbeae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out shape after PatchMerging_1: torch.Size([2, 56, 56, 96])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "  def __init__(self, fn):\n",
        "    super().__init__()\n",
        "    self.fn = fn\n",
        "\n",
        "  def forward(self, x, **kwargs):\n",
        "    return self.fn(x, **kwargs) + x"
      ],
      "metadata": {
        "id": "u_58XF4d71Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreNorm(nn.Module):\n",
        "  def __init__(self, dim, fn, version=2):\n",
        "    super().__init__()\n",
        "    self.fn = fn\n",
        "    self.norm = nn.LayerNorm(dim)\n",
        "    self.version = version\n",
        "\n",
        "  def forward(self, x, **kwargs):\n",
        "    '''\n",
        "    In SWIN transformers V1, they use Norm, which do the Normalization\n",
        "    before the block,\n",
        "\n",
        "    In SWIN transformers V2, they use Norm, which do the Normalization\n",
        "    after the block.\n",
        "    '''\n",
        "    if self.version == 2:\n",
        "      return self.norm(self.fn(x), **kwargs)\n",
        "    return self.fn(self.norm(x), **kwargs)"
      ],
      "metadata": {
        "id": "FB8IFQy58UDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  # mlp_dim = hidden_dim * 4 where dim=hidden_dim=(96, 192, 384, 768)\n",
        "  def __init__(self, dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(dim, hidden_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(hidden_dim, dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "1YZSdDqB_Qdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Swin Block\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1fqzDh6Hj-C3l5CD4LwuwrrAByNo6_8Xd)\n",
        "\n",
        "The first part in red is the WindowAttention and the second Part is MlpBlock, which is implemented in `FeedForward` class. LN is just a LayerNorm implemented in `PreNorm` class. And each of them has a **Residual connnection**."
      ],
      "metadata": {
        "id": "nuHWIkCm6Q5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Window Attention\n",
        "\n",
        "The most important class of the whole SWIN Transformer implementation\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1YgesmeyPdAB1vsQb9GCODI3b5_P5Y2lr)\n"
      ],
      "metadata": {
        "id": "6l6tRQK3Zo6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Shifting\n",
        "\n",
        "When `self.shifted` is `True` we are going to shift all of the windows to right and down at the same time by `window_size \\\\ 2`. But whe we shift the regions on the right and down will be empty and we need to pad them.\n",
        "\n",
        "In the paper they suggest 2 types of padding: `naive padding` and `cyclic padding`. **Naive padding** just add `zeros` at those positions. **Cyclic Padding**:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1LrSzZ6ZC65mOofeOsEKO6FKMnvMksZWE)\n",
        "\n",
        "</br>\n",
        "\n",
        "Padding speed:\n",
        "![](https://drive.google.com/uc?export=view&id=1IzWGcsnDJLTGw-4hxOCavAAW40P2fMtz)\n",
        "\n",
        "**!NOTE:** So, shifts should be negative for `right & down` direction and positive for `left & up` direction.\n",
        "\n",
        "With this type of padding (**Cyclic padding**) we have a problem with the last row and last column. After the shifting and padding, the first row from the original image will be neighbour with the last row from the original image. The same applies for the first and last columns. So we need to do something to handle this problem. This is called **Masking**.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1PkmGJX6A9SiDbGlRPK6w3H3GlR2Vj9Mp)\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1vNhXyd3RHx2hr1DDeDheTMdyrHUzrvk_)"
      ],
      "metadata": {
        "id": "T1SyjmM_bMQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CyclicShift(nn.Module):\n",
        "  def __init__(self, displacement):\n",
        "    super().__init__()\n",
        "    self.displacement = displacement\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))"
      ],
      "metadata": {
        "id": "B20ayILXe6M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.linspace(1, 16, 16).view(4, 4)\n",
        "print(x)\n",
        "\n",
        "y = torch.roll(x, shifts=(-1, -1), dims=(0, 1))\n",
        "print(y)\n",
        "\n",
        "x = torch.roll(x, shifts=(1, 1), dims=(0, 1))\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2MkEEA4f8UO",
        "outputId": "22c2ade4-855a-4604-d923-933ac6b5aed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.,  2.,  3.,  4.],\n",
            "        [ 5.,  6.,  7.,  8.],\n",
            "        [ 9., 10., 11., 12.],\n",
            "        [13., 14., 15., 16.]])\n",
            "tensor([[ 6.,  7.,  8.,  5.],\n",
            "        [10., 11., 12.,  9.],\n",
            "        [14., 15., 16., 13.],\n",
            "        [ 2.,  3.,  4.,  1.]])\n",
            "tensor([[16., 13., 14., 15.],\n",
            "        [ 4.,  1.,  2.,  3.],\n",
            "        [ 8.,  5.,  6.,  7.],\n",
            "        [12.,  9., 10., 11.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(window_size, displacement, upper_lower, left_right):\n",
        "  mask = torch.zeros(window_size ** 2, window_size ** 2) # 49x49\n",
        "\n",
        "  if upper_lower:\n",
        "    mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
        "    mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
        "\n",
        "  if left_right:\n",
        "    mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
        "    mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
        "    mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
        "    mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
        "\n",
        "  return mask"
      ],
      "metadata": {
        "id": "uuuzreFpqC0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_upper_lower = create_mask(3, 1, False, True)\n",
        "print(mask_upper_lower)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wfv_vhYsh_H",
        "outputId": "e781fec7-7535-4da3-a1a5-f69c7532c7c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
            "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
            "        [-inf, -inf, 0., -inf, -inf, 0., -inf, -inf, 0.],\n",
            "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
            "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
            "        [-inf, -inf, 0., -inf, -inf, 0., -inf, -inf, 0.],\n",
            "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
            "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
            "        [-inf, -inf, 0., -inf, -inf, 0., -inf, -inf, 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relative positional embeddings\n",
        "\n",
        "With relative positional embedding we reduce the number of parameters from **(n * n)** to **(2*window_size - 1) * (2*window_size - 1)**.\n",
        "\n",
        "We can achieve that by:\n",
        "\n",
        "`self.pos_embeddings = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))`\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1uo2JhjAISwybYW-8FPoQd5FxTrVh5WLQ)\n",
        "\n",
        "### Performance comparation:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=19M_OHqdWn3nVr_1f8XhWsQfl0GUK6ZPM)"
      ],
      "metadata": {
        "id": "HxDBDk63QqS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def get_relative_distances(window_size: int):\n",
        "  indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
        "  distances = indices[None, :, :] - indices[:, None, :]\n",
        "  return distances"
      ],
      "metadata": {
        "id": "T-b-WWk1vngr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rel_dist = get_relative_distances(3)\n",
        "print(f'Shape: {rel_dist.shape}')\n",
        "print(f'First: {rel_dist[2,1,:]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owwyTrv7wk5v",
        "outputId": "324bd343-9fd6-4157-8d3e-9971cae3b622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: torch.Size([9, 9, 2])\n",
            "First: tensor([ 0, -1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Exampme of relative positional embedding (2, 2)\n",
        "# pos embedding\n",
        "p = torch.tensor([[1, 2],\n",
        "                  [3, 4]])\n",
        "\n",
        "print(f'p.size: {p.size()}')\n",
        "\n",
        "# relative indices (2 * 2 - 1, 2 * 2 - 1, 2)\n",
        "rel_pos_embed = torch.tensor([[[0, 0], [0, 0], [0, 0]],\n",
        "                              [[1, 1], [1, 1], [1, 1]],\n",
        "                              [[0, 1], [0, 1], [0, 1]]])\n",
        "print(f'rel_pos_embed.size: {rel_pos_embed.size()}')\n",
        "\n",
        "print(p[rel_pos_embed[:, :, 0], rel_pos_embed[:, :, 1]])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_KdwQnGunYt",
        "outputId": "4cc21190-654a-43d9-ebe7-5a7766128ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p.size: torch.Size([2, 2])\n",
            "rel_pos_embed.size: torch.Size([3, 3, 2])\n",
            "tensor([[1, 1, 1],\n",
            "        [4, 4, 4],\n",
            "        [2, 2, 2]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version 2 of SWIN transformers changes\n",
        "\n",
        "In Version 2 of SWIN transformers the authors introduces **cosine similarity** instead of just **dot product** between Q and K matrices. Then the result is divided by TAU which is **traineable parameter** which is defined with value > 0.01.\n",
        "\n"
      ],
      "metadata": {
        "id": "X51kZ3vtCeau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowAttention(nn.Module):\n",
        "  def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embeddings):\n",
        "    super().__init__()\n",
        "    # dim = hidden_dim = (96, 192, 384, 768)\n",
        "    # heads = num_heads = (3, 6, 12, 24)\n",
        "    # head_dim = 32\n",
        "    inner_dim = head_dim * heads  # 32 * 3 = 96, 6 * 32 = 192, 12 * 32 = 384, 24 * 32 = 768\n",
        "    self.heads = heads\n",
        "    self.scale = head_dim ** -.5  # scalling dot product inside the softmax\n",
        "    self.window_size = window_size # by default it is 7\n",
        "    self.relative_pos_embeddings = relative_pos_embeddings\n",
        "    self.shifted = shifted\n",
        "\n",
        "    # The TAU parameter for Version 2 cosine similarity\n",
        "    self.tau = nn.Parameter(torch.tensor(.01), requires_grad=True)\n",
        "\n",
        "    '''\n",
        "    If shifted is True, we are going to shift all of the windows to the right and down and pad them.\n",
        "    '''\n",
        "    if self.shifted:\n",
        "      displacement = window_size // 2   # 7 // 2 = 3\n",
        "      self.cyclic_shift = CyclicShift(-displacement)\n",
        "      self.cyclic_back_shift = CyclicShift(displacement)\n",
        "\n",
        "      # (49, 49) masks are NOT learneable paramenters; set requires_grad to False\n",
        "      self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size,\n",
        "                                                       displacement=displacement,\n",
        "                                                       upper_lower=True,\n",
        "                                                       left_right=False),\n",
        "                                           requires_grad=False)\n",
        "      self.left_right_mask = nn.Parameter(create_mask(window_size=window_size,\n",
        "                                                      displacement=displacement,\n",
        "                                                      upper_lower=False,\n",
        "                                                      left_right=True),\n",
        "                                          requires_grad=False)\n",
        "    # Queries, Keys, Values\n",
        "    self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "    # dim = (96, 192, 384, 768) and inner_dim = head_dim * heads; we can C * 3 and gives us same thing\n",
        "\n",
        "    if relative_pos_embeddings:\n",
        "      self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
        "      self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
        "      # (13, 13) because if I am one cell I have 6 possible relationship behind and after\n",
        "    else:\n",
        "      # Absolute positional embedding\n",
        "      self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2)) # (49, 49)\n",
        "\n",
        "    # inner_dim = head_dim * heads = C, dim = hidden_dim = (96, 192, 384, 768)\n",
        "    self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.shifted:\n",
        "      # print(f'x.size(): {x.size()}')    # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "      x = self.cyclic_shift(x)\n",
        "      # print(f'x.size(): {x.size()}')    # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "\n",
        "    # x.shape is batch_size, height, width, channels\n",
        "    batch_size, height, width, _ = x.shape\n",
        "    heads = self.heads\n",
        "\n",
        "    # Window sizes do NOT change, but number of channels is increased 3 times, for Queries, Keys and Values\n",
        "    # print(f'self.to_qkv(x): {self.to_qkv(x).size()}')   # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96*3, 192*3, 384*3, 768*3))\n",
        "\n",
        "    # Chunk function divide it to 3 distinct tensors (Q, K and V) based on last dim, which is number of channels\n",
        "    qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "\n",
        "    # print(f'qkv[0]: {qkv[0].size()}')   # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768)) for qkv[0] as qkv is a tuple of 3 elements\n",
        "\n",
        "    # Number of windows in height\n",
        "    n_wh = height // self.window_size     # (56 // 7 = 8, 28 // 7 = 4, 14 // 7 = 2, 7 // 7 = 1)\n",
        "    # Number of windows in width\n",
        "    n_ww = width // self.window_size      # (56 // 7 = 8, 28 // 7 = 4, 14 // 7 = 2, 7 // 7 = 1)\n",
        "\n",
        "    q, k, v = map(lambda t: rearrange(t, 'b (n_wh w_h) (n_ww w_w) (h d) -> b h (n_wh n_ww) (w_h w_w) d',\n",
        "                                      h = heads, w_h = self.window_size, w_w = self.window_size),\n",
        "                  qkv)\n",
        "\n",
        "    # print(f'q.size(): {q.size()}')\n",
        "    # (b=1, h=(3, 6, 12, 24), (n_wh * n_ww)=(64, 16, 4, 1), (w_h*w_w)=49, d=32) where d=head_dim, h=number of heads\n",
        "\n",
        "    # print(f'k.size(): {k.size()}')    # same as q\n",
        "    # print(f'v.size(): {v.size()}')    # same as v\n",
        "\n",
        "    # Find dot product of Q and K\n",
        "    # Dot product similarity for version 1 of SWIN transformers\n",
        "\n",
        "    #dots = einsum(q, k, 'b h w i d, b h w j d -> b h w i j') * self.scale\n",
        "\n",
        "    # b - batch size, h - heads (3, 6, 12, 24), w - windows (64, 16, 4, 1) i=j=49\n",
        "\n",
        "    # Cosine similarity\n",
        "    # First normalize Q and K matrices with respect to each row\n",
        "    q = F.normalize(q, p=2.0, dim=-1)\n",
        "    k = F.normalize(k, p=2.0, dim=-1)\n",
        "\n",
        "    # cosine similarity divided by self.tau\n",
        "    dots = einsum(q, k, 'b h w i d, b h w j d -> b h w i j') / self.tau\n",
        "    # b-batch size, h-heads, w-windows, i = j = 49\n",
        "\n",
        "    # Now we should add the Positional Embedding\n",
        "    if self.relative_pos_embeddings:\n",
        "      # print(f'self.pos_embedding.size(): {self.pos_embedding.size()}')    # (13, 13)\n",
        "      tmp1 = self.relative_indices[:, :, 0] # (49, 49) as relative_indices is (49, 49, 2)\n",
        "\n",
        "      # tmp2 = self.pos_embedding[self.relative_indices[:, :, 0]], self.relative_indices[:, :, 1]     # (49, 49)\n",
        "      dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]      # b-batch_size, h-#heads, w-windows, i=49, j=49\n",
        "    else:\n",
        "      # Absolute embeddings\n",
        "      dots += self.pos_embedding   # (b-batch_size, h-heads, w-windows, i=49, j=49)\n",
        "\n",
        "    # Add masking\n",
        "    if self.shifted:\n",
        "      # tmp1 = self.upper_lower_mask    (49, 49)\n",
        "      # tmp2 = self.left_right_mask     (49, 49)\n",
        "\n",
        "      # Add mask to last row\n",
        "      dots[:, :, -n_ww:] += self.upper_lower_mask\n",
        "      # Add mask to last column\n",
        "      dots[:, :, n_ww-1::n_ww] += self.left_right_mask\n",
        "\n",
        "    attn = dots.softmax(dim=-1) # (batch_size, heads=(3, 6, 12, 24), windows=(64, 16, 4, 1), i=49, j=49)\n",
        "\n",
        "    # Add the Value to the Attention\n",
        "    out = einsum(attn, v, 'b h w i j, b h w j d -> b h w i d')\n",
        "    # shape: (batch_size, heads=(3, 6, 12, 24), windows=(64, 16, 4, 1), i=49, d=head_dim=32)\n",
        "\n",
        "    # Rearrange output\n",
        "    out = rearrange(out, 'b h (n_wh n_ww) (w_h w_w) d -> b (n_wh w_h) (n_ww w_w) (h d)',\n",
        "                    h=heads, w_h=self.window_size, w_w=self.window_size, n_wh=n_wh, n_ww=n_ww)\n",
        "    # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "\n",
        "    out = self.to_out(out) # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "\n",
        "    if self.shifted:\n",
        "      # We need to shift back to get to the original shape to send it to the next block\n",
        "      out = self.cyclic_back_shift(out)    # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "EBy-PC8xZha7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test WindowAttention"
      ],
      "metadata": {
        "id": "LetV-ujlqDdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_attn = WindowAttention(dim=96, heads=3, head_dim=32, shifted=True, window_size=7, relative_pos_embeddings=True)\n",
        "\n",
        "dummy_in = torch.randn(2, 56, 56, 96)\n",
        "\n",
        "out = w_attn(dummy_in)\n",
        "print(f'Out shape: {out.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ynrb_WdqG27",
        "outputId": "a5975e05-0084-443e-c456-55871ecdb251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out shape: torch.Size([2, 56, 56, 96])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinBlock(nn.Module):\n",
        "  def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embeddings):\n",
        "    # dim=hidden_dim=(96, 192, 384, 768), heads=num_heads=(3, 6, 12, 24), mlp_dim=dim*4\n",
        "    super().__init__()\n",
        "    self.attention_block = Residual(PreNorm(dim, WindowAttention(\n",
        "        dim=dim,\n",
        "        heads=heads,\n",
        "        head_dim=head_dim,\n",
        "        shifted=shifted,\n",
        "        window_size=window_size,\n",
        "        relative_pos_embeddings=relative_pos_embeddings\n",
        "    )))\n",
        "\n",
        "    self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.attention_block(x)  # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "    x = self.mlp_block(x) # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "grKfkqgl3IMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test SwinBlock"
      ],
      "metadata": {
        "id": "cZ16kxwltpSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sb = SwinBlock(dim=96, heads=3, head_dim=32, mlp_dim=96*4, shifted=False,\n",
        "               window_size=7, relative_pos_embeddings=True)\n",
        "\n",
        "dummy_in = torch.randn(2, 56, 56, 96)\n",
        "\n",
        "out = sb(dummy_in)\n",
        "print(f'Out shape: {out.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rju5Okoatsgi",
        "outputId": "a4034c63-1958-4160-9ec1-7ed153371752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out shape: torch.Size([2, 56, 56, 96])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class StageModule(nn.Module):\n",
        "  def __init__(self, in_channels, hidden_dim, layers, downscalling_factor,\n",
        "               num_heads, head_dim, window_size, relative_pos_embeddings):\n",
        "    super().__init__()\n",
        "    assert layers % 2 == 0, 'Stage Layers must be divisible by 2 for regular and shifted blocks'\n",
        "\n",
        "    self.patch_partition = PatchMerging_Conv(in_channels=in_channels, out_channels=hidden_dim,\n",
        "                                             downscalling_factor=downscalling_factor)\n",
        "\n",
        "    self.layers = nn.ModuleList([])\n",
        "    for _ in range(layers // 2):\n",
        "      self.layers.append(nn.ModuleList([\n",
        "          SwinBlock(dim=hidden_dim, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dim*4,\n",
        "                    shifted=False, window_size=window_size, relative_pos_embeddings=relative_pos_embeddings),\n",
        "          SwinBlock(dim=hidden_dim, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dim*4,\n",
        "                    shifted=True, window_size=window_size, relative_pos_embeddings=relative_pos_embeddings)\n",
        "      ]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    What does this shapes at the end mean? It is a tuple of 4 elements.\n",
        "    First element is the batch size - 1.\n",
        "\n",
        "    The next element (3, 96, 192, 384) is the number of channels is every SWIN transformer block.\n",
        "      In block 0 it is 3, in block 1 96, in block 2 192 and the last block 384.\n",
        "\n",
        "    The next element (224, 56, 28, 14) is the height of the image in every SWIN transformer block.\n",
        "      In block 0 it is 224, block 1 - 56, block 2 - 28 and last block 14.\n",
        "\n",
        "    The next element (224, 56, 28, 14) is the width of the image in every SWIN transformer block.\n",
        "      In block 0 it is 224, block 1 - 56, block 2 - 28 and last block 14.\n",
        "\n",
        "    So the input of StageModule forward method on first block will be (batch_size, 3, 224, 224),\n",
        "      the second block (batch_size, 96, 56, 56), the third block (batch_size, 192, 28, 28)\n",
        "      and tha last (batch_size, 384, 14, 14)\n",
        "\n",
        "    '''\n",
        "\n",
        "    # print(f'Before patch partition: {x.size()}')    # (1, (3, 96, 192, 384), (224, 56, 28, 14), (224, 56, 28, 14))\n",
        "    x = self.patch_partition(x)\n",
        "    # print(f'After patch partition: {x.size()}')     # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "    for regular_block, shifted_block in self.layers:\n",
        "      x = regular_block(x) # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "      x = shifted_block(x) # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "\n",
        "    x = x.permute(0, 3, 1, 2) # (1, 768, 7, 7)\n",
        "    return x"
      ],
      "metadata": {
        "id": "vBnLx6iDAi2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SWINTransformer(nn.Module):\n",
        "  def __init__(self, *, hidden_dim, layers, heads, channels=3, num_classes=1000,\n",
        "               head_dim=32, window_size=7, downscaling_factors=(4, 2, 2, 2),\n",
        "               relative_pos_embeddings=True):\n",
        "    super().__init__()\n",
        "    self.stage1 = StageModule(in_channels=channels, hidden_dim=hidden_dim, layers=layers[0],\n",
        "                              downscalling_factor=downscaling_factors[0], num_heads=heads[0],\n",
        "                              head_dim=head_dim, window_size=window_size, relative_pos_embeddings=relative_pos_embeddings)\n",
        "\n",
        "    self.stage2 = StageModule(in_channels=hidden_dim, hidden_dim=hidden_dim * 2, layers=layers[1],\n",
        "                              downscalling_factor=downscaling_factors[1], num_heads=heads[1],\n",
        "                              head_dim=head_dim, window_size=window_size, relative_pos_embeddings=relative_pos_embeddings)\n",
        "\n",
        "    self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dim=hidden_dim * 4, layers=layers[2],\n",
        "                              downscalling_factor=downscaling_factors[2], num_heads=heads[2],\n",
        "                              head_dim=head_dim, window_size=window_size, relative_pos_embeddings=relative_pos_embeddings)\n",
        "\n",
        "    self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dim=hidden_dim * 8, layers=layers[3],\n",
        "                              downscalling_factor=downscaling_factors[3], num_heads=heads[3],\n",
        "                              head_dim=head_dim, window_size=window_size, relative_pos_embeddings=relative_pos_embeddings)\n",
        "\n",
        "    self.mlp_head = nn.Sequential(\n",
        "        nn.LayerNorm(hidden_dim*8),\n",
        "        nn.Linear(hidden_dim * 8, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, img):\n",
        "    x = self.stage1(img)\n",
        "    x = self.stage2(x)\n",
        "    x = self.stage3(x)\n",
        "    x = self.stage4(x)\n",
        "\n",
        "    x = x.mean(dim=[2, 3])\n",
        "\n",
        "    x = self.mlp_head(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "NkUBKBC43AI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "  return SWINTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)"
      ],
      "metadata": {
        "id": "6hZX5qAS_Xm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = swin_t(\n",
        "    hidden_dim=96,\n",
        "    layers=(2, 2, 6, 2),\n",
        "    heads=(3, 6, 12, 24),\n",
        "    channels=3,\n",
        "    num_classes=3,\n",
        "    head_dim=32,\n",
        "    window_size=7,\n",
        "    downscaling_factors=(4, 2, 2, 2),\n",
        "    relative_pos_embeddings=True\n",
        ")\n",
        "\n",
        "dummy_h = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "loggits = net(dummy_h)\n",
        "print(loggits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vv_JDjKmoOB",
        "outputId": "dca42141-5ca2-472b-e0c5-81a69524a016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.2585, -0.5348, -1.1451]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    }
  ]
}